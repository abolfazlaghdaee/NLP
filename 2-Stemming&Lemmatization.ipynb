{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e74a397-f5c6-4b31-a03a-38f2e805d54e",
   "metadata": {},
   "source": [
    "#### Second Session: Stemming and Lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "063c2a68-badd-4ae2-ac01-78bd831fff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem.lancaster import *\n",
    "from nltk. stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e8e19f4-7449-47aa-8a86-52479e547935",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['connection', 'connecting', 'disconnect', 'connect', 'connected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aee173c-328d-4a36-af2e-6b09338f75cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "150bf4ad-b676-4d51-bdea-3cb06f835097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection --> connect\n",
      "connecting --> connect\n",
      "disconnect --> disconnect\n",
      "connect --> connect\n",
      "connected --> connect\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token + \" --> \" + stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6572f6ad-e41b-401b-bb24-5af3af3d3da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lacaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eba073f0-794c-407f-91b1-294f16388182",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['lovely', 'decenteralized',' better', 'information', 'disable', 'did'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ebabee9-0d4d-4dc6-a249-dbf38c31c299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lovely --> love\n",
      "lovely --> lov\n",
      "\n",
      "\n",
      "decenteralized --> decenter\n",
      "decenteralized --> dec\n",
      "\n",
      "\n",
      " better -->  better\n",
      " better -->  better\n",
      "\n",
      "\n",
      "information --> inform\n",
      "information --> inform\n",
      "\n",
      "\n",
      "disable --> disabl\n",
      "disable --> dis\n",
      "\n",
      "\n",
      "did --> did\n",
      "did --> did\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in tokens: \n",
    "    print(token + \" --> \" + stemmer.stem(token))\n",
    "    print(token + \" --> \" + lacaster.stem(token))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "188007d8-bb40-44e1-ae55-ca344bd1355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d747f7a6-1dc2-443a-99b2-690ecb6067c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lovely --> love\n",
      "lovely --> lov\n",
      "Lemma --> lovely\n",
      "\n",
      "\n",
      "decenteralized --> decenter\n",
      "decenteralized --> dec\n",
      "Lemma --> decenteralized\n",
      "\n",
      "\n",
      " better -->  better\n",
      " better -->  better\n",
      "Lemma -->  better\n",
      "\n",
      "\n",
      "information --> inform\n",
      "information --> inform\n",
      "Lemma --> information\n",
      "\n",
      "\n",
      "disable --> disabl\n",
      "disable --> dis\n",
      "Lemma --> disable\n",
      "\n",
      "\n",
      "did --> did\n",
      "did --> did\n",
      "Lemma --> do\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in tokens: \n",
    "    print(token + \" --> \" + stemmer.stem(token))\n",
    "    print(token + \" --> \" + lacaster.stem(token))\n",
    "    print('Lemma' + \" --> \" + lemmatizer.lemmatize(token, pos =\"v\"))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477dd843-a4f9-46f7-9afd-6083f87732a8",
   "metadata": {},
   "source": [
    "#### in Persian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa8f5483-bfe8-4991-a3ca-3339c329ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0cfcb9df-d4f8-4f0d-b43f-7337d25098d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_tokens = [\"رفت\" , \"می روم\", \"رفتی\", \"کتاب ها\", \"کتابخوانی\", \"کتابم\", \"کتابی\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea17554f-5587-4430-af3c-220c0801f820",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = hazm.Normalizer()\n",
    "stemmer = hazm.Stemmer()\n",
    "lemma = hazm.Lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "002f9cae-8538-4a81-8d0c-0a28c3608d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem--> رف\n",
      "lemma--> رفت#رو\n",
      "\n",
      "\n",
      "stem--> می‌رو\n",
      "lemma--> رفت#رو\n",
      "\n",
      "\n",
      "stem--> رفت\n",
      "lemma--> رفت#رو\n",
      "\n",
      "\n",
      "stem--> کتاب\n",
      "lemma--> کتاب\n",
      "\n",
      "\n",
      "stem--> کتابخوان\n",
      "lemma--> کتابخوانی\n",
      "\n",
      "\n",
      "stem--> کتاب\n",
      "lemma--> کتاب\n",
      "\n",
      "\n",
      "stem--> کتاب\n",
      "lemma--> کتابی\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in per_tokens:\n",
    "    token  = normalize.normalize(token)\n",
    "    print('stem' + '--> ' + stemmer.stem(token))\n",
    "    print('lemma' + '--> ' + lemma.lemmatize(token))\n",
    "    print('\\n')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
